{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOV60wCRGKj+R+pzgJLGE85",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yossi122/COVID_Project/blob/master/Modeling_and_Optimization_of_Epidemiological_Control.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXU06i3JCJH9"
      },
      "outputs": [],
      "source": [
        "from scipy.integrate import solve_ivp\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "MEMORY_SIZE = 7000000\n",
        "memory = deque(maxlen=MEMORY_SIZE) #STORAGE\n",
        "reward_memory_total = deque(maxlen=MEMORY_SIZE)\n",
        "terminal_memory_total = deque(maxlen=MEMORY_SIZE)\n",
        "\n",
        "\n",
        "class Simulation:\n",
        "\n",
        "  def __init__(self, beta, sigma, gamma, mu, susceptible_population, exposed_population, infected_population, recovered_population, dead_population, current_economy):\n",
        "    self.beta = beta #transmission rate\n",
        "    self.sigma = sigma #incubation rate\n",
        "    self.gamma = gamma #recovery rate\n",
        "    self.mu = mu #death rate\n",
        "    self.susceptible_population = susceptible_population\n",
        "    self.exposed_population = exposed_population\n",
        "    self.infected_population = infected_population\n",
        "    self.recovered_population = recovered_population\n",
        "    self.dead_population = dead_population\n",
        "    self.current_economy = current_economy\n",
        "    self.economic_impact_from_actions = 1\n",
        "    self.day = [0,1]\n",
        "    self.population_size = self.susceptible_population + self.infected_population\n",
        "\n",
        "  def seir_f(self, t, y, beta, sigma, gamma, mu):\n",
        "        s, e, i, r, d = y\n",
        "        N = self.population_size\n",
        "        return np.array([(-beta * i * s)/N,\n",
        "                         -sigma * e + (beta * i * s)/N,\n",
        "                         -(gamma * i) -(mu * i)+ sigma * e,\n",
        "                         (gamma) * i,\n",
        "                         (mu * i)])\n",
        "\n",
        "  def step(self):\n",
        "    y = [self.susceptible_population, self.exposed_population, self.infected_population, self.recovered_population, self.dead_population] #current state\n",
        "\n",
        "    sol = solve_ivp(self.seir_f, self.day, y, args=(self.beta, self.sigma, self.gamma, self.mu),t_eval = self.day)\n",
        "\n",
        "    self.day[0] += 1\n",
        "    self.day[1] += 1\n",
        "\n",
        "    #new state\n",
        "    self.susceptible_population, self.exposed_population, self.infected_population, self.recovered_population, self.dead_population = sol.y.T[1][0], sol.y.T[1][1], sol.y.T[1][2],sol.y.T[1][3],sol.y.T[1][4]\n",
        "    self.current_economy = (self.susceptible_population + self.exposed_population + self.recovered_population)*self.economic_impact_from_actions\n",
        "\n",
        "  def run(self, run_type = 'natural'):\n",
        "    if run_type == 'natural':\n",
        "      i = 0\n",
        "      while self.infected_population >= 1:\n",
        "        i+=1\n",
        "        self.step()\n",
        "      print(i)\n",
        "    else:\n",
        "      for i in range(run_type):\n",
        "        self.step()\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.model, self.target = None, None\n",
        "    self.discount_factor = 0.95\n",
        "    self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    self.loss_fn = tf.keras.losses.mean_squared_error\n",
        "    self.time_steps = 30\n",
        "    self.input_shape = 7 #SEIRDE + Action\n",
        "    self.n_outputs = 4\n",
        "\n",
        "  def set_model(self):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True), input_shape=(30,7)),\n",
        "        keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True)),\n",
        "        keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True)),\n",
        "        keras.layers.Dense(128, activation = 'relu'),\n",
        "        keras.layers.Dense(64, activation = 'relu'),\n",
        "        keras.layers.Dense(32, activation = 'relu'),\n",
        "        keras.layers.Dense(self.n_outputs, activation = 'softmax')\n",
        "    ])\n",
        "    self.model = model\n",
        "    self.model.compile(loss=self.loss_fn, optimizer = self.optimizer)\n",
        "    self.target=model\n",
        "\n",
        "  def epsilon_greedy_policy(self,state, epsilon):\n",
        "    #small probability of exploring - epsilon is probability of exploration, np.random.rand() gives # between 0,1\n",
        "    if np.random.rand() < epsilon:\n",
        "      return random.randint(0,self.n_outputs-1)\n",
        "    #predict next action using model\n",
        "    else:\n",
        "      state_array = np.array(state)\n",
        "      state_array = state_array.reshape(1, 30, 7)\n",
        "      Q_values = self.model.predict(state_array)\n",
        "      prediction = np.argmax(Q_values[0], axis=1)[-1]\n",
        "      return prediction\n",
        "\n",
        "  def calculate_reward(self,current_economy,dead_population,infected_population, total):\n",
        "    r = 12 #weight of economy vs cases\n",
        "    s = 5 #weight of deaths\n",
        "    #economy\n",
        "    Et = current_economy/total\n",
        "    #deaths\n",
        "    Dt = s* (dead_population/total)\n",
        "    #active cases\n",
        "    At = -r * (infected_population/total) * 100\n",
        "    #REWARD FUNCTION\n",
        "    reward =  Et * math.e**(At)+Dt\n",
        "    return reward\n",
        "\n",
        "  def update_model(self,current_states, next_states, model_history):\n",
        "    rewards = []\n",
        "    terminals = []\n",
        "\n",
        "    for i in range(1,31):\n",
        "      rewards.append(reward_memory_total[-i])#last 30 days\n",
        "      terminals.append(terminal_memory_total[-i])\n",
        "\n",
        "    next_Q_values = (self.model.predict(next_states))[0] #predict next\n",
        "    all_Q_values = (self.model.predict(current_states))[0] #predict i-1\n",
        "    prediction = (self.target.predict(next_states))[0] #use target to find next\n",
        "    #ALL ARE (1,30,4)\n",
        "\n",
        "    ACTION_HISTORY = []\n",
        "\n",
        "    for i in range(30):\n",
        "      ACTION_HISTORY.append(current_states[0][i][6]) #retrieve actions\n",
        "\n",
        "    best_next_actions = []\n",
        "    for i in range(len(next_Q_values)):\n",
        "      best_next_actions.append(np.argmax(next_Q_values[i]))\n",
        "\n",
        "    next_mask = tf.one_hot(best_next_actions, self.n_outputs).numpy()\n",
        "\n",
        "    next_best_Q_values = (prediction * next_mask).sum(axis=1)\n",
        "\n",
        "    target_Q_values = []\n",
        "\n",
        "    for i in range(30):\n",
        "      target_Q_values.append(rewards[i] + (1-terminals[i]) * self.discount_factor * next_best_Q_values[i])\n",
        "      action = ACTION_HISTORY[i] #action taken at every step\n",
        "      all_Q_values[i][int(action)] = target_Q_values[i] #this is like \"informal\" mask\n",
        "\n",
        "    all_Q_values = [all_Q_values.tolist()]\n",
        "    all_Q_values = np.asarray(all_Q_values)\n",
        "\n",
        "    history = self.model.fit(current_states, all_Q_values, verbose = 0) #calculate loss, update weights\n",
        "    model_history.append(history.history['loss'])\n",
        "\n",
        "\n",
        "def get_current_states():\n",
        "  current_states = []\n",
        "  for i in range(1,31):\n",
        "    current_states.append(memory[-i])\n",
        "  current_states = np.array(current_states[::-1])\n",
        "  current_states = current_states[np.newaxis,...]\n",
        "  return current_states\n",
        "\n",
        "\n",
        "def get_previous_states():\n",
        "  previous_states = []\n",
        "  for i in range(2,32):\n",
        "    previous_states.append(memory[-i])\n",
        "  previous_states = np.array(previous_states[::-1])\n",
        "  previous_states = previous_states[np.newaxis,...]\n",
        "  return previous_states\n",
        "\n",
        "def execute_action(sim, action):\n",
        "  if action == 0: #none\n",
        "    sim.beta = beta\n",
        "    sim.sigma = sigma\n",
        "    sim.gamma = gamma\n",
        "    sim.mu = mu\n",
        "    sim.economic_impact_from_actions = 1\n",
        "\n",
        "  if action == 1: #social distance\n",
        "    factor = 0.5\n",
        "    sim.beta = beta*factor\n",
        "    sim.sigma = sigma*factor\n",
        "    sim.gamma = gamma*factor\n",
        "    sim.mu = mu *factor\n",
        "    sim.economic_impact_from_actions = factor\n",
        "\n",
        "  if action == 2: #lockdown\n",
        "    factor = 0.25\n",
        "    sim.beta = beta*factor\n",
        "    sim.sigma = sigma*factor\n",
        "    sim.gamma = gamma*factor\n",
        "    sim.mu = mu*factor\n",
        "    sim.economic_impact_from_actions = factor\n",
        "\n",
        "  if action == 3: #curfew + lockdown\n",
        "    factor = 0.15\n",
        "    sim.beta = beta*factor\n",
        "    sim.sigma = sigma*factor\n",
        "    sim.gamma = gamma*factor\n",
        "    sim.mu = mu*factor\n",
        "    sim.economic_impact_from_actions = factor\n",
        "\n",
        "agent = Agent('agent_1')\n",
        "agent.set_model()\n",
        "num_episodes = 200\n",
        "\n",
        "cum_reward_graph = []\n",
        "\n",
        "model_history = []\n",
        "\n",
        "#da super loop\n",
        "\n",
        "for episode in tqdm(range(num_episodes)):\n",
        "  #values for simulation when t=0, set according to real-world data\n",
        "  # beta = random.randint(25,30)*0.01 #infection rate\n",
        "  # sigma = random.randint(30,50)*0.01 #incubation rate\n",
        "  # gamma = random.randint(10,30)*0.01 # recovery rate\n",
        "  # mu=random.randint(15,20)*0.001 #mortality rate\n",
        "  # population = random.randint(200,1000) #population\n",
        "  # E0, I0, R0, D0 = 0,random.randint(10,60)*population*0.01,0, 0 #infected can be 10-60% of the population\n",
        "  beta = 0.12 #infection\n",
        "  sigma = 1 #incubation\n",
        "  gamma = (1/27) #recovery\n",
        "  mu = 0.009  #mortality\n",
        "  population = 1000\n",
        "  E0, I0, R0, D0 = 0, 70, 0, 0\n",
        "  S0 = population - E0 - I0 - R0 - D0\n",
        "  econ_0 = S0\n",
        "  #initialize simulation\n",
        "  sim = Simulation(beta,sigma,gamma,mu,S0,E0,I0,R0,D0,econ_0)\n",
        "\n",
        "  #cumulative reward\n",
        "  cumulative_reward = 0\n",
        "\n",
        "  #run for a 30 day buffer first time through\n",
        "  if episode == 0:\n",
        "    for i in range(31):\n",
        "      sim.run(run_type = 1)\n",
        "      memory.append([sim.susceptible_population, sim.exposed_population, sim.infected_population, sim.recovered_population, sim.dead_population, sim.current_economy, 0])\n",
        "      reward = agent.calculate_reward(sim.current_economy, sim.dead_population, sim.infected_population,sim.population_size)\n",
        "      cumulative_reward += reward\n",
        "      reward_memory_total.append(reward)\n",
        "      terminal_memory_total.append(0)\n",
        "  #calculate epsilon for policy\n",
        "  epsilon = max(1- (episode/200),0.01)\n",
        "\n",
        "  while sim.infected_population >= 1:\n",
        "\n",
        "    sim.run(run_type = 1)\n",
        "\n",
        "    current_states = get_current_states() #update 30 day list of SEIRDE\n",
        "    if episode == 0 and sim.day[0] == 32:\n",
        "      pass\n",
        "    else:\n",
        "      previous_states = get_previous_states()\n",
        "      agent.update_model(previous_states, current_states, model_history)\n",
        "\n",
        "    action = agent.epsilon_greedy_policy(current_states, epsilon) #ADD ACTION EFFECT ON SIM\n",
        "    memory.append([sim.susceptible_population, sim.exposed_population, sim.infected_population, sim.recovered_population, sim.dead_population, sim.current_economy,action])\n",
        "\n",
        "    reward = agent.calculate_reward(sim.current_economy, sim.dead_population, sim.infected_population, sim.population_size)\n",
        "    cumulative_reward += reward\n",
        "    reward_memory_total.append(reward)\n",
        "\n",
        "    execute_action(sim, action)\n",
        "\n",
        "    if sim.infected_population >= 1:\n",
        "      terminal_memory_total.append(0)\n",
        "\n",
        "  terminal_memory_total.append(1)\n",
        "  cum_reward_graph.append(cumulative_reward*(1/sim.day[0]))\n",
        "\n",
        "#graph all information\n",
        "\n",
        "\n",
        "plt.plot(model_history) #loss\n",
        "print(model_history)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(cum_reward_graph) #reward\n",
        "print(cum_reward_graph)\n",
        "plt.show()\n",
        "print('we have finished (woohoo!)')\n",
        "\n",
        "#save model\n",
        "version = 1\n",
        "fn = (\"C:/Users/ishir/OneDrive/Documents/Model_Weights/\" + str(version))\n",
        "agent.model.save_weights(fn)\n",
        "\n",
        "#TEST MODEL\n",
        "#values for simulation when t=0, set according to real-world data\n",
        "beta = 0.12\n",
        "sigma = 1\n",
        "gamma = (1/27)\n",
        "mu = 0.009\n",
        "population = 1000\n",
        "E0, I0, R0, D0 = 0, 70, 0, 0\n",
        "S0 = population - E0 - I0 - R0 - D0\n",
        "econ_0 = S0\n",
        "#initialize simulation\n",
        "sim = Simulation(beta,sigma,gamma,mu,S0,E0,I0,R0,D0,econ_0)\n",
        "\n",
        "action_hist = []\n",
        "susceptible_hist = []\n",
        "exposed_hist = []\n",
        "infected_hist = []\n",
        "recovered_hist = []\n",
        "death_hist = []\n",
        "economy_hist = []\n",
        "reward_hist = []\n",
        "\n",
        "epsilon = 0 #always use model\n",
        "\n",
        "#doesn't update model\n",
        "\n",
        "while sim.infected_population >= 1:\n",
        "  sim.run(run_type = 1)\n",
        "\n",
        "  current_states = get_current_states()\n",
        "\n",
        "  print(sim.day[0])\n",
        "\n",
        "  action = agent.epsilon_greedy_policy(current_states, epsilon)\n",
        "  action_hist.append(action)\n",
        "\n",
        "  susceptible_hist.append(sim.susceptible_population)\n",
        "  exposed_hist.append(sim.exposed_population)\n",
        "  infected_hist.append(sim.infected_population)\n",
        "  recovered_hist.append(sim.recovered_population)\n",
        "  death_hist.append(sim.dead_population)\n",
        "  economy_hist.append(sim.current_economy)\n",
        "  memory.append([sim.susceptible_population, sim.exposed_population, sim.infected_population, sim.recovered_population, sim.dead_population, sim.current_economy, action])\n",
        "\n",
        "  reward = agent.calculate_reward(sim.current_economy, sim.dead_population, sim.infected_population, sim.population_size)\n",
        "  reward_hist.append(reward)\n",
        "\n",
        "  execute_action(sim, action)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.grid()\n",
        "ax.margins(0)\n",
        "\n",
        "ax.plot(susceptible_hist, label = 'susceptible')\n",
        "ax.plot(exposed_hist, label = 'exposed')\n",
        "ax.plot(infected_hist, label = 'infected')\n",
        "ax.plot(recovered_hist, label = 'recovered')\n",
        "ax.plot(death_hist, label = 'deaths')\n",
        "ax.plot(economy_hist, label = 'economy')\n",
        "\n",
        "x_new_start = 0\n",
        "\n",
        "for i in range(len(action_hist)-1):\n",
        "  current = action_hist[i]\n",
        "  next =  action_hist[i+1]\n",
        "  print(current)\n",
        "\n",
        "  if current == 0:\n",
        "    color = 'blue'\n",
        "  elif current == 1:\n",
        "    color = 'green'\n",
        "  elif current == 2:\n",
        "    color = 'yellow'\n",
        "  elif current == 3:\n",
        "    color = 'red'\n",
        "  ax.axvspan(i, i+1, facecolor = color, alpha = 0.4)\n",
        "\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(reward_hist)\n",
        "plt.show()\n",
        "\n",
        "def run_action(action_type, color):\n",
        "  #TEST MODEL\n",
        "  #values for simulation when t=0, set according to real-world data\n",
        "  beta = 0.12\n",
        "  sigma = 1\n",
        "  gamma = (1/27)\n",
        "  mu = 0.009\n",
        "  population = 1000\n",
        "  E0, I0, R0, D0 = 0, 70, 0, 0\n",
        "  S0 = population - E0 - I0 - R0 - D0\n",
        "  econ_0 = S0\n",
        "  #initialize simulation\n",
        "  sim = Simulation(beta,sigma,gamma,mu,S0,E0,I0,R0,D0,econ_0)\n",
        "\n",
        "  action_hist = []\n",
        "  susceptible_hist = []\n",
        "  exposed_hist = []\n",
        "  infected_hist = []\n",
        "  recovered_hist = []\n",
        "  death_hist = []\n",
        "  economy_hist = []\n",
        "  reward_hist = []\n",
        "\n",
        "  while sim.infected_population >= 1:\n",
        "    sim.run(run_type = 1)\n",
        "\n",
        "    action_hist.append(action_type)\n",
        "\n",
        "    susceptible_hist.append(sim.susceptible_population)\n",
        "    exposed_hist.append(sim.exposed_population)\n",
        "    infected_hist.append(sim.infected_population)\n",
        "    recovered_hist.append(sim.recovered_population)\n",
        "    death_hist.append(sim.dead_population)\n",
        "    economy_hist.append(sim.current_economy)\n",
        "\n",
        "    reward = agent.calculate_reward(sim.current_economy, sim.dead_population, sim.infected_population, sim.population_size)\n",
        "    reward_hist.append(reward)\n",
        "\n",
        "    execute_action(sim, action_type)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.grid()\n",
        "  ax.margins(0)\n",
        "\n",
        "  ax.plot(susceptible_hist, label = 'susceptible')\n",
        "  ax.plot(exposed_hist, label = 'exposed')\n",
        "  ax.plot(infected_hist, label = 'infected')\n",
        "  ax.plot(recovered_hist, label = 'recovered')\n",
        "  ax.plot(death_hist, label = 'deaths')\n",
        "  ax.plot(economy_hist, label = 'economy')\n",
        "\n",
        "  ax.axvspan(0, sim.day[1], facecolor = color, alpha = 0.4)\n",
        "\n",
        "  ax.legend()\n",
        "  plt.show()\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  ax.plot(reward_hist)\n",
        "  plt.show()\n",
        "\n",
        "  return infected_hist\n",
        "\n",
        "none = run_action(0, 'blue')\n",
        "one = run_action(1,'green')\n",
        "two = run_action(2,'yellow')\n",
        "three = run_action(3, 'red')\n",
        "\n",
        "plt.plot(none)\n",
        "plt.plot(one)\n",
        "plt.plot(two)\n",
        "plt.plot(three)"
      ]
    }
  ]
}